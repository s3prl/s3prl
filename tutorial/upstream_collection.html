<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Use Problem module to run customizable recipes" href="problem.html" /><link rel="prev" title="Install S3PRL" href="installation.html" />

    <!-- Generated with Sphinx 5.1.1 and Furo 2023.03.27 -->
        <title>S3PRL Upstream Collection - S3PRL 0.4.18 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">S3PRL 0.4.18
 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">S3PRL 0.4.18
 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Install S3PRL</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">S3PRL Upstream Collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="problem.html">Use Problem module to run customizable recipes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How to Contribute</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute/general.html">General Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/upstream.html">Adding New Upstream</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../_autosummary/s3prl.nn.html">nn</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.beam_decoder.html">beam_decoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.common.html">common</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.hear.html">hear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.interface.html">interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.linear.html">linear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.pit.html">pit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.pooling.html">pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.rnn.html">rnn</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.speaker_loss.html">speaker_loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.speaker_model.html">speaker_model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.specaug.html">specaug</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.nn.upstream.html">upstream</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../_autosummary/s3prl.problem.html">problem</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.problem.asr.html">asr</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.asr.run.html">run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.asr.superb_asr.html">superb_asr</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.asr.superb_pr.html">superb_pr</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.asr.superb_sf.html">superb_sf</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.problem.asv.html">asv</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.asv.run.html">run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.asv.superb_asv.html">superb_asv</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.problem.base.html">base</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.problem.common.html">common</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.example.html">example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_beijing_opera.html">hear_beijing_opera</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_cremad.html">hear_cremad</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_dcase_2016_task2.html">hear_dcase_2016_task2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_esc50.html">hear_esc50</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_fsd.html">hear_fsd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_gsc5hr.html">hear_gsc5hr</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_gtzan.html">hear_gtzan</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_gtzan_music_speech.html">hear_gtzan_music_speech</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_gunshot.html">hear_gunshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_libricount.html">hear_libricount</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_maestro.html">hear_maestro</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_nsynth5hr.html">hear_nsynth5hr</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_stroke.html">hear_stroke</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_tonic.html">hear_tonic</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_vocal.html">hear_vocal</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.hear_vox_lingual.html">hear_vox_lingual</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.run.html">run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.superb_er.html">superb_er</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.superb_ic.html">superb_ic</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.superb_ks.html">superb_ks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.common.superb_sid.html">superb_sid</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.problem.diarization.html">diarization</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" role="switch" type="checkbox"/><label for="toctree-checkbox-6"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.diarization.run.html">run</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.diarization.superb_sd.html">superb_sd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.problem.diarization.util.html">util</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../_autosummary/s3prl.task.html">task</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" role="switch" type="checkbox"/><label for="toctree-checkbox-7"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.base.html">base</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.diarization.html">diarization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.dump_feature.html">dump_feature</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.event_prediction.html">event_prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.scene_prediction.html">scene_prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.speaker_verification_task.html">speaker_verification_task</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.speech2text_ctc_task.html">speech2text_ctc_task</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.task.utterance_classification_task.html">utterance_classification_task</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../_autosummary/s3prl.dataio.html">dataio</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" role="switch" type="checkbox"/><label for="toctree-checkbox-8"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.dataio.collate_fn.html">collate_fn</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.html">corpus</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" role="switch" type="checkbox"/><label for="toctree-checkbox-9"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.base.html">base</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.fluent_speech_commands.html">fluent_speech_commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.iemocap.html">iemocap</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.librilight.html">librilight</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.librispeech.html">librispeech</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.quesst14.html">quesst14</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.snips.html">snips</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.speech_commands.html">speech_commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.voxceleb1sid.html">voxceleb1sid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.corpus.voxceleb1sv.html">voxceleb1sv</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.html">dataset</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" role="switch" type="checkbox"/><label for="toctree-checkbox-10"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.base.html">base</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.diarization.html">diarization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.encode.html">encode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.frame_label.html">frame_label</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.load_audio.html">load_audio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.dataset.util.html">util</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.dataio.encoder.html">encoder</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" role="switch" type="checkbox"/><label for="toctree-checkbox-11"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.encoder.category.html">category</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.encoder.g2p.html">g2p</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.encoder.tokenizer.html">tokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.encoder.vocabulary.html">vocabulary</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.html">sampler</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" role="switch" type="checkbox"/><label for="toctree-checkbox-12"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.balanced_weighted_sampler.html">balanced_weighted_sampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.distributed_sampler.html">distributed_sampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.fixed_batch_size_batch_sampler.html">fixed_batch_size_batch_sampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.group_same_item_sampler.html">group_same_item_sampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.max_timestamp_batch_sampler.html">max_timestamp_batch_sampler</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/s3prl.dataio.sampler.sorted_sampler.html">sorted_sampler</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../_autosummary/s3prl.metric.html">metric</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" role="switch" type="checkbox"/><label for="toctree-checkbox-13"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.metric.common.html">common</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.metric.diarization.html">diarization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.metric.slot_filling.html">slot_filling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../_autosummary/s3prl.util.html">util</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" role="switch" type="checkbox"/><label for="toctree-checkbox-14"><div class="visually-hidden">Toggle child pages in navigation</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.util.audio_info.html">audio_info</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.util.benchmark.html">benchmark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.util.download.html">download</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.util.override.html">override</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.util.pseudo_data.html">pseudo_data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/s3prl.util.seed.html">seed</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="s3prl-upstream-collection">
<h1>S3PRL Upstream Collection<a class="headerlink" href="#s3prl-upstream-collection" title="Permalink to this heading">#</a></h1>
<p>We collect almost all the existing SSL pre-trained models in S3PRL,
so you can import and use them easily in an unified I/O interface.</p>
<p><a class="reference internal" href="../_autosummary/s3prl.nn.upstream.html#s3prl.nn.upstream.S3PRLUpstream" title="s3prl.nn.upstream.S3PRLUpstream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">s3prl.nn.upstream.S3PRLUpstream</span></code></a> is an easy interface to retrieve all the self-supervised learning (SSL) pre-trained models
available in S3PRL. the <code class="code docutils literal notranslate"><span class="pre">name</span></code> argument for <a class="reference internal" href="../_autosummary/s3prl.nn.upstream.html#s3prl.nn.upstream.S3PRLUpstream" title="s3prl.nn.upstream.S3PRLUpstream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">s3prl.nn.upstream.S3PRLUpstream</span></code></a> specifies the checkpoint,
and then the pre-trained models in this checkpoint will be automatically constructed and
initialized.</p>
<p>Here is an example on how to get a hubert model and its representation using the <code class="code docutils literal notranslate"><span class="pre">name='hubert'</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">s3prl.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">S3PRLUpstream</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">S3PRLUpstream</span><span class="p">(</span><span class="s2">&quot;hubert&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">wavs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16000</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">wavs_len</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">16000</span> <span class="o">*</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16000</span> <span class="o">*</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">all_hs</span><span class="p">,</span> <span class="n">all_hs_len</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">wavs</span><span class="p">,</span> <span class="n">wavs_len</span><span class="p">)</span>

<span class="k">for</span> <span class="n">hs</span><span class="p">,</span> <span class="n">hs_len</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_hs</span><span class="p">,</span> <span class="n">all_hs_len</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hs_len</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">)</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hs</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">hs_len</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For each SSL learning method, like wav2vec 2.0, there are several checkpoint variants, trained by
different amount of unlabeled data, or different model sizes. Hence there are also various
<code class="code docutils literal notranslate"><span class="pre">name</span></code> to retrieve these different models.</p>
<p>Like, the HuBERT method has “hubert” and “hubert_large_ll60k” different names for different
checkpoint variants.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Some SSL pre-trained models’ entries can be further configured by a <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code> dictionary.
See <a class="reference internal" href="../_autosummary/s3prl.nn.html#s3prl.nn.S3PRLUpstream" title="s3prl.nn.S3PRLUpstream"><code class="xref py py-obj docutils literal notranslate"><span class="pre">s3prl.nn.S3PRLUpstream</span></code></a>. You can find the valid <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code> options in each SSL
model category. If not documented, by default it does not support any <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>.</p>
</div>
<p>The following includes the model and checkpoint information for each <code class="code docutils literal notranslate"><span class="pre">name</span></code>, including the releasing date,
paper, citation, model architecture, pre-training data, criterion, and their source code. The format follows:</p>
<section id="ssl-method">
<h2>SSL Method<a class="headerlink" href="#ssl-method" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/">Paper full title with arxiv link</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>citation-block,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Paper<span class="w"> </span>Title<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Authors<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">month</span><span class="o">={</span>May<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The information shared across checkpoint variants.</p>
<section id="name1">
<h3>name1<a class="headerlink" href="#name1" title="Permalink to this heading">#</a></h3>
<p>The detailed specific information for this checkpoint variant (<code class="code docutils literal notranslate"><span class="pre">name=name1</span></code>)</p>
</section>
<section id="name2">
<h3>name2<a class="headerlink" href="#name2" title="Permalink to this heading">#</a></h3>
<p>The detailed specific information for this checkpoint variant (<code class="code docutils literal notranslate"><span class="pre">name=name2</span></code>)</p>
</section>
</section>
<section id="mockingjay">
<h2>Mockingjay<a class="headerlink" href="#mockingjay" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1910.12638">Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>mockingjay,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Mockingjay:<span class="w"> </span>Unsupervised<span class="w"> </span>Speech<span class="w"> </span>Representation<span class="w"> </span>Learning<span class="w"> </span>with<span class="w"> </span>Deep<span class="w"> </span>Bidirectional<span class="w"> </span>Transformer<span class="w"> </span>Encoders<span class="o">}</span>,
<span class="w">    </span><span class="nv">ISBN</span><span class="o">={</span><span class="m">9781509066315</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">url</span><span class="o">={</span>http://dx.doi.org/10.1109/ICASSP40776.2020.9054458<span class="o">}</span>,
<span class="w">    </span><span class="nv">DOI</span><span class="o">={</span><span class="m">10</span>.1109/icassp40776.2020.9054458<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>ICASSP<span class="w"> </span><span class="m">2020</span><span class="w"> </span>-<span class="w"> </span><span class="m">2020</span><span class="w"> </span>IEEE<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Acoustics,<span class="w"> </span>Speech<span class="w"> </span>and<span class="w"> </span>Signal<span class="w"> </span>Processing<span class="w"> </span><span class="o">(</span>ICASSP<span class="o">)}</span>,
<span class="w">    </span><span class="nv">publisher</span><span class="o">={</span>IEEE<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Liu,<span class="w"> </span>Andy<span class="w"> </span>T.<span class="w"> </span>and<span class="w"> </span>Yang,<span class="w"> </span>Shu-wen<span class="w"> </span>and<span class="w"> </span>Chi,<span class="w"> </span>Po-Han<span class="w"> </span>and<span class="w"> </span>Hsu,<span class="w"> </span>Po-chun<span class="w"> </span>and<span class="w"> </span>Lee,<span class="w"> </span>Hung-yi<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">month</span><span class="o">={</span>May<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>Mockingjay is a BERT on Spectrogram, with 12-layers of transformer encoders in the paper.</p>
<section id="id1">
<h3>mockingjay<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#mockingjay-origin">mockingjay_origin</a></p>
</section>
<section id="mockingjay-origin">
<h3>mockingjay_origin<a class="headerlink" href="#mockingjay-origin" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#mockingjay-logmellinearlarge-t-adamw-b32-500k-360hr-drop1">mockingjay_logMelLinearLarge_T_AdamW_b32_500k_360hr_drop1</a></p>
</section>
<section id="mockingjay-100hr">
<h3>mockingjay_100hr<a class="headerlink" href="#mockingjay-100hr" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#mockingjay-logmelbase-t-adamw-b32-200k-100hr">mockingjay_logMelBase_T_AdamW_b32_200k_100hr</a></p>
</section>
<section id="mockingjay-960hr">
<h3>mockingjay_960hr<a class="headerlink" href="#mockingjay-960hr" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#mockingjay-logmelbase-t-adamw-b32-1m-960hr-drop1">mockingjay_logMelBase_T_AdamW_b32_1m_960hr_drop1</a></p>
</section>
<section id="mockingjay-logmelbase-t-adamw-b32-200k-100hr">
<h3>mockingjay_logMelBase_T_AdamW_b32_200k_100hr<a class="headerlink" href="#mockingjay-logmelbase-t-adamw-b32-200k-100hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 200k</p></li>
<li><p>Unlabled Speech: LibriSpeech 100hr</p></li>
</ul>
</section>
<section id="mockingjay-logmellinearlarge-t-adamw-b32-500k-360hr-drop1">
<h3>mockingjay_logMelLinearLarge_T_AdamW_b32_500k_360hr_drop1<a class="headerlink" href="#mockingjay-logmellinearlarge-t-adamw-b32-500k-360hr-drop1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel (input) / 201-dim Linear (target)</p></li>
<li><p>Alteration: time</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 500k</p></li>
<li><p>Unlabled Speech: LibriSpeech 360hr</p></li>
</ul>
</section>
<section id="mockingjay-logmelbase-t-adamw-b32-1m-960hr">
<h3>mockingjay_logMelBase_T_AdamW_b32_1m_960hr<a class="headerlink" href="#mockingjay-logmelbase-t-adamw-b32-1m-960hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
<section id="mockingjay-logmelbase-t-adamw-b32-1m-960hr-drop1">
<h3>mockingjay_logMelBase_T_AdamW_b32_1m_960hr_drop1<a class="headerlink" href="#mockingjay-logmelbase-t-adamw-b32-1m-960hr-drop1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
<li><p>Differences: Dropout of 0.1 (instead of 0.3)</p></li>
</ul>
</section>
<section id="mockingjay-logmelbase-t-adamw-b32-1m-960hr-seq3k">
<h3>mockingjay_logMelBase_T_AdamW_b32_1m_960hr_seq3k<a class="headerlink" href="#mockingjay-logmelbase-t-adamw-b32-1m-960hr-seq3k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
<li><p>Differences: sequence length of 3k (instead of 1.5k)</p></li>
</ul>
</section>
</section>
<section id="tera">
<h2>TERA<a class="headerlink" href="#tera" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2007.06028">TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@misc<span class="o">{</span>tera,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>TERA:<span class="w"> </span>Self-Supervised<span class="w"> </span>Learning<span class="w"> </span>of<span class="w"> </span>Transformer<span class="w"> </span>Encoder<span class="w"> </span>Representation<span class="w"> </span><span class="k">for</span><span class="w"> </span>Speech<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Andy<span class="w"> </span>T.<span class="w"> </span>Liu<span class="w"> </span>and<span class="w"> </span>Shang-Wen<span class="w"> </span>Li<span class="w"> </span>and<span class="w"> </span>Hung-yi<span class="w"> </span>Lee<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">eprint</span><span class="o">={</span><span class="m">2007</span>.06028<span class="o">}</span>,
<span class="w">    </span><span class="nv">archivePrefix</span><span class="o">={</span>arXiv<span class="o">}</span>,
<span class="w">    </span><span class="nv">primaryClass</span><span class="o">={</span>eess.AS<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id2">
<h3>tera<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#tera-960hr">tera_960hr</a></p>
</section>
<section id="tera-100hr">
<h3>tera_100hr<a class="headerlink" href="#tera-100hr" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#tera-logmelbase-t-f-m-adamw-b32-200k-100hr">tera_logMelBase_T_F_M_AdamW_b32_200k_100hr</a></p>
</section>
<section id="tera-960hr">
<h3>tera_960hr<a class="headerlink" href="#tera-960hr" title="Permalink to this heading">#</a></h3>
<p>This is alias for <a class="reference internal" href="#tera-logmelbase-t-f-m-adamw-b32-1m-960hr-drop1">tera_logMelBase_T_F_M_AdamW_b32_1m_960hr_drop1</a></p>
</section>
<section id="tera-logmelbase-t-f-adamw-b32-200k-100hr">
<h3>tera_logMelBase_T_F_AdamW_b32_200k_100hr<a class="headerlink" href="#tera-logmelbase-t-f-adamw-b32-200k-100hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time + freq</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 200k</p></li>
<li><p>Unlabled Speech: LibriSpeech 100hr</p></li>
</ul>
</section>
<section id="tera-logmelbase-t-f-m-adamw-b32-200k-100hr">
<h3>tera_logMelBase_T_F_M_AdamW_b32_200k_100hr<a class="headerlink" href="#tera-logmelbase-t-f-m-adamw-b32-200k-100hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time + freq + mag</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 200k</p></li>
<li><p>Unlabled Speech: LibriSpeech 100hr</p></li>
</ul>
</section>
<section id="tera-logmelbase-t-f-adamw-b32-1m-960hr">
<h3>tera_logMelBase_T_F_AdamW_b32_1m_960hr<a class="headerlink" href="#tera-logmelbase-t-f-adamw-b32-1m-960hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time + freq</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
<section id="tera-logmelbase-t-f-adamw-b32-1m-960hr-drop1">
<h3>tera_logMelBase_T_F_AdamW_b32_1m_960hr_drop1<a class="headerlink" href="#tera-logmelbase-t-f-adamw-b32-1m-960hr-drop1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time + freq</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
<li><p>Differences: Dropout of 0.1 (instead of 0.3)</p></li>
</ul>
</section>
<section id="tera-logmelbase-t-f-adamw-b32-1m-960hr-seq3k">
<h3>tera_logMelBase_T_F_AdamW_b32_1m_960hr_seq3k<a class="headerlink" href="#tera-logmelbase-t-f-adamw-b32-1m-960hr-seq3k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time + freq</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
<li><p>Differences: sequence length of 3k (instead of 1.5k)</p></li>
</ul>
</section>
<section id="tera-logmelbase-t-f-m-adamw-b32-1m-960hr-drop1">
<h3>tera_logMelBase_T_F_M_AdamW_b32_1m_960hr_drop1<a class="headerlink" href="#tera-logmelbase-t-f-m-adamw-b32-1m-960hr-drop1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time + freq + mag</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: 960hr</p></li>
<li><p>Differences: Dropout of 0.1 (instead of 0.3)</p></li>
</ul>
</section>
<section id="tera-fbankbase-t-f-adamw-b32-200k-100hr">
<h3>tera_fbankBase_T_F_AdamW_b32_200k_100hr<a class="headerlink" href="#tera-fbankbase-t-f-adamw-b32-200k-100hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 240-dim fbank</p></li>
<li><p>Alteration: time + freq</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 200k</p></li>
<li><p>Unlabled Speech: LibriSpeech 100hr</p></li>
</ul>
</section>
</section>
<section id="audio-albert">
<h2>Audio ALBERT<a class="headerlink" href="#audio-albert" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2007.06028">Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chi2021audio,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Audio<span class="w"> </span>albert:<span class="w"> </span>A<span class="w"> </span>lite<span class="w"> </span>bert<span class="w"> </span><span class="k">for</span><span class="w"> </span>self-supervised<span class="w"> </span>learning<span class="w"> </span>of<span class="w"> </span>audio<span class="w"> </span>representation<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Chi,<span class="w"> </span>Po-Han<span class="w"> </span>and<span class="w"> </span>Chung,<span class="w"> </span>Pei-Hung<span class="w"> </span>and<span class="w"> </span>Wu,<span class="w"> </span>Tsung-Han<span class="w"> </span>and<span class="w"> </span>Hsieh,<span class="w"> </span>Chun-Cheng<span class="w"> </span>and<span class="w"> </span>Chen,<span class="w"> </span>Yen-Hao<span class="w"> </span>and<span class="w"> </span>Li,<span class="w"> </span>Shang-Wen<span class="w"> </span>and<span class="w"> </span>Lee,<span class="w"> </span>Hung-yi<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span><span class="m">2021</span><span class="w"> </span>IEEE<span class="w"> </span>Spoken<span class="w"> </span>Language<span class="w"> </span>Technology<span class="w"> </span>Workshop<span class="w"> </span><span class="o">(</span>SLT<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">344</span>--350<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id3">
<h3>audio_albert<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#audio-albert-960hr">audio_albert_960hr</a></p>
</section>
<section id="audio-albert-960hr">
<h3>audio_albert_960hr<a class="headerlink" href="#audio-albert-960hr" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#audio-albert-logmelbase-t-share-adamw-b32-1m-960hr-drop1">audio_albert_logMelBase_T_share_AdamW_b32_1m_960hr_drop1</a></p>
</section>
<section id="audio-albert-logmelbase-t-share-adamw-b32-1m-960hr-drop1">
<h3>audio_albert_logMelBase_T_share_AdamW_b32_1m_960hr_drop1<a class="headerlink" href="#audio-albert-logmelbase-t-share-adamw-b32-1m-960hr-drop1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature: 80-dim log Mel</p></li>
<li><p>Alteration: time</p></li>
<li><p>Optimizer: AdamW</p></li>
<li><p>Batch size: 32</p></li>
<li><p>Total steps: 1M</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="apc">
<h2>APC<a class="headerlink" href="#apc" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1904.03240">An Unsupervised Autoregressive Model for Speech Representation Learning</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chung2019unsupervised,
<span class="w">    </span><span class="nv">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>An<span class="w"> </span>unsupervised<span class="w"> </span>autoregressive<span class="w"> </span>model<span class="w"> </span><span class="k">for</span><span class="w"> </span>speech<span class="w"> </span>representation<span class="w"> </span>learning<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Chung,<span class="w"> </span>Yu-An<span class="w"> </span>and<span class="w"> </span>Hsu,<span class="w"> </span>Wei-Ning<span class="w"> </span>and<span class="w"> </span>Tang,<span class="w"> </span>Hao<span class="w"> </span>and<span class="w"> </span>Glass,<span class="w"> </span>James<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Interspeech<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="m">2019</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id4">
<h3>apc<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#apc-360hr">apc_360hr</a></p>
</section>
<section id="apc-360hr">
<h3>apc_360hr<a class="headerlink" href="#apc-360hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 360hr</p></li>
</ul>
</section>
<section id="apc-960hr">
<h3>apc_960hr<a class="headerlink" href="#apc-960hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="vq-apc">
<h2>VQ-APC<a class="headerlink" href="#vq-apc" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2005.08392">Vector-Quantized Autoregressive Predictive Coding</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chung2020vqapc,
<span class="w">    </span><span class="nv">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Vector-quantized<span class="w"> </span>autoregressive<span class="w"> </span>predictive<span class="w"> </span>coding<span class="o">}</span>,
<span class="w">    </span><span class="nv">autohor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Chung,<span class="w"> </span>Yu-An<span class="w"> </span>and<span class="w"> </span>Tang,<span class="w"> </span>Hao<span class="w"> </span>and<span class="w"> </span>Glass,<span class="w"> </span>James<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Interspeech<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="m">2020</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id5">
<h3>vq_apc<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#vq-apc-360hr">vq_apc_360hr</a></p>
</section>
<section id="vq-apc-360hr">
<h3>vq_apc_360hr<a class="headerlink" href="#vq-apc-360hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 360hr</p></li>
</ul>
</section>
<section id="vq-apc-960hr">
<h3>vq_apc_960hr<a class="headerlink" href="#vq-apc-960hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="npc">
<h2>NPC<a class="headerlink" href="#npc" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2011.00406">Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>liu2020nonautoregressive,
<span class="w">    </span><span class="nv">title</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Non-Autoregressive<span class="w"> </span>Predictive<span class="w"> </span>Coding<span class="w"> </span><span class="k">for</span><span class="w"> </span>Learning<span class="w"> </span>Speech<span class="w"> </span>Representations<span class="w"> </span>from<span class="w"> </span>Local<span class="w"> </span>Dependencies<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Liu,<span class="w"> </span>Alexander<span class="w"> </span>and<span class="w"> </span>Chung,<span class="w"> </span>Yu-An<span class="w"> </span>and<span class="w"> </span>Glass,<span class="w"> </span>James<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2011.00406<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="o">{</span><span class="m">2020</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id6">
<h3>npc<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#npc-360hr">npc_360hr</a></p>
</section>
<section id="npc-360hr">
<h3>npc_360hr<a class="headerlink" href="#npc-360hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 360hr</p></li>
</ul>
</section>
<section id="npc-960hr">
<h3>npc_960hr<a class="headerlink" href="#npc-960hr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="pase">
<h2>PASE+<a class="headerlink" href="#pase" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2001.09239">Multi-task self-supervised learning for Robust Speech Recognition</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>ravanelli2020multi,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Multi-task<span class="w"> </span>self-supervised<span class="w"> </span>learning<span class="w"> </span><span class="k">for</span><span class="w"> </span>robust<span class="w"> </span>speech<span class="w"> </span>recognition<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Ravanelli,<span class="w"> </span>Mirco<span class="w"> </span>and<span class="w"> </span>Zhong,<span class="w"> </span>Jianyuan<span class="w"> </span>and<span class="w"> </span>Pascual,<span class="w"> </span>Santiago<span class="w"> </span>and<span class="w"> </span>Swietojanski,<span class="w"> </span>Pawel<span class="w"> </span>and<span class="w"> </span>Monteiro,<span class="w"> </span>Joao<span class="w"> </span>and<span class="w"> </span>Trmal,<span class="w"> </span>Jan<span class="w"> </span>and<span class="w"> </span>Bengio,<span class="w"> </span>Yoshua<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>ICASSP<span class="w"> </span><span class="m">2020</span>-2020<span class="w"> </span>IEEE<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Acoustics,<span class="w"> </span>Speech<span class="w"> </span>and<span class="w"> </span>Signal<span class="w"> </span>Processing<span class="w"> </span><span class="o">(</span>ICASSP<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">6989</span>--6993<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>To use PASE models, there are many extra dependencies required to install.
Please follow the below installation instruction:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>https://raw.githubusercontent.com/s3prl/s3prl/master/s3prl/upstream/pase/requirements.txt
</pre></div>
</div>
</div>
<section id="pase-plus">
<h3>pase_plus<a class="headerlink" href="#pase-plus" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 50hr</p></li>
</ul>
</section>
</section>
<section id="modified-cpc">
<h2>Modified CPC<a class="headerlink" href="#modified-cpc" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2002.02848">Unsupervised pretraining transfers well across languages</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>riviere2020unsupervised,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Unsupervised<span class="w"> </span>pretraining<span class="w"> </span>transfers<span class="w"> </span>well<span class="w"> </span>across<span class="w"> </span>languages<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Riviere,<span class="w"> </span>Morgane<span class="w"> </span>and<span class="w"> </span>Joulin,<span class="w"> </span>Armand<span class="w"> </span>and<span class="w"> </span>Mazar<span class="o">{</span><span class="se">\&#39;</span>e<span class="o">}</span>,<span class="w"> </span>Pierre-Emmanuel<span class="w"> </span>and<span class="w"> </span>Dupoux,<span class="w"> </span>Emmanuel<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>ICASSP<span class="w"> </span><span class="m">2020</span>-2020<span class="w"> </span>IEEE<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Acoustics,<span class="w"> </span>Speech<span class="w"> </span>and<span class="w"> </span>Signal<span class="w"> </span>Processing<span class="w"> </span><span class="o">(</span>ICASSP<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">7414</span>--7418<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a slightly improved version on the original CPC by DeepMind. To cite the DeepMind version:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>oord2018representation,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Representation<span class="w"> </span>learning<span class="w"> </span>with<span class="w"> </span>contrastive<span class="w"> </span>predictive<span class="w"> </span>coding<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Oord,<span class="w"> </span>Aaron<span class="w"> </span>van<span class="w"> </span>den<span class="w"> </span>and<span class="w"> </span>Li,<span class="w"> </span>Yazhe<span class="w"> </span>and<span class="w"> </span>Vinyals,<span class="w"> </span>Oriol<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:1807.03748<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2018</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
</div>
<section id="id7">
<h3>modified_cpc<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriLight 60k hours</p></li>
</ul>
</section>
</section>
<section id="decoar">
<h2>DeCoAR<a class="headerlink" href="#decoar" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1912.01679">Deep contextualized acoustic representations for semi-supervised speech recognition</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>ling2020deep,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Deep<span class="w"> </span>contextualized<span class="w"> </span>acoustic<span class="w"> </span>representations<span class="w"> </span><span class="k">for</span><span class="w"> </span>semi-supervised<span class="w"> </span>speech<span class="w"> </span>recognition<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Ling,<span class="w"> </span>Shaoshi<span class="w"> </span>and<span class="w"> </span>Liu,<span class="w"> </span>Yuzong<span class="w"> </span>and<span class="w"> </span>Salazar,<span class="w"> </span>Julian<span class="w"> </span>and<span class="w"> </span>Kirchhoff,<span class="w"> </span>Katrin<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>ICASSP<span class="w"> </span><span class="m">2020</span>-2020<span class="w"> </span>IEEE<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Acoustics,<span class="w"> </span>Speech<span class="w"> </span>and<span class="w"> </span>Signal<span class="w"> </span>Processing<span class="w"> </span><span class="o">(</span>ICASSP<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">6429</span>--6433<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="decoar-layers">
<h3>decoar_layers<a class="headerlink" href="#decoar-layers" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="decoar-2-0">
<h2>DeCoAR 2.0<a class="headerlink" href="#decoar-2-0" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2012.06659">DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@misc<span class="o">{</span>ling2020decoar,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>DeCoAR<span class="w"> </span><span class="m">2</span>.0:<span class="w"> </span>Deep<span class="w"> </span>Contextualized<span class="w"> </span>Acoustic<span class="w"> </span>Representations<span class="w"> </span>with<span class="w"> </span>Vector<span class="w"> </span>Quantization<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Shaoshi<span class="w"> </span>Ling<span class="w"> </span>and<span class="w"> </span>Yuzong<span class="w"> </span>Liu<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">eprint</span><span class="o">={</span><span class="m">2012</span>.06659<span class="o">}</span>,
<span class="w">    </span><span class="nv">archivePrefix</span><span class="o">={</span>arXiv<span class="o">}</span>,
<span class="w">    </span><span class="nv">primaryClass</span><span class="o">={</span>eess.AS<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="decoar2">
<h3>decoar2<a class="headerlink" href="#decoar2" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="wav2vec">
<h2>wav2vec<a class="headerlink" href="#wav2vec" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1904.05862">wav2vec: Unsupervised Pre-Training for Speech Recognition</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>schneider2019wav2vec,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>wav2vec:<span class="w"> </span>Unsupervised<span class="w"> </span>Pre-Training<span class="w"> </span><span class="k">for</span><span class="w"> </span>Speech<span class="w"> </span>Recognition<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Schneider,<span class="w"> </span>Steffen<span class="w"> </span>and<span class="w"> </span>Baevski,<span class="w"> </span>Alexei<span class="w"> </span>and<span class="w"> </span>Collobert,<span class="w"> </span>Ronan<span class="w"> </span>and<span class="w"> </span>Auli,<span class="w"> </span>Michael<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>Proc.<span class="w"> </span>Interspeech<span class="w"> </span><span class="m">2019</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">3465</span>--3469<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2019</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id8">
<h3>wav2vec<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#wav2vec-large">wav2vec_large</a></p>
</section>
<section id="wav2vec-large">
<h3>wav2vec_large<a class="headerlink" href="#wav2vec-large" title="Permalink to this heading">#</a></h3>
<p>This is the official wav2vec model from fairseq.</p>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="vq-wav2vec">
<h2>vq-wav2vec<a class="headerlink" href="#vq-wav2vec" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1910.05453">vq-wav2vec: Self-supervised learning of discrete speech representations</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>baevski2019vq,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>vq-wav2vec:<span class="w"> </span>Self-Supervised<span class="w"> </span>Learning<span class="w"> </span>of<span class="w"> </span>Discrete<span class="w"> </span>Speech<span class="w"> </span>Representations<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Baevski,<span class="w"> </span>Alexei<span class="w"> </span>and<span class="w"> </span>Schneider,<span class="w"> </span>Steffen<span class="w"> </span>and<span class="w"> </span>Auli,<span class="w"> </span>Michael<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Learning<span class="w"> </span>Representations<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2019</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We only take the Conv encoders’ hidden_states for vq-wav2vec in this SSL method category.
If you wish to consider the BERT model after ths Conv encoders, please refer to <a class="reference internal" href="#discrete-bert">Discrete BERT</a>.</p>
</div>
<section id="id9">
<h3>vq_wav2vec<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#vq-wav2vec-gumbel">vq_wav2vec_gumbel</a></p>
</section>
<section id="vq-wav2vec-gumbel">
<h3>vq_wav2vec_gumbel<a class="headerlink" href="#vq-wav2vec-gumbel" title="Permalink to this heading">#</a></h3>
<p>This is the official vq-wav2vec model from fairseq.
This model uses gumbel-softmax as the quantization technique</p>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
<section id="vq-wav2vec-kmeans">
<h3>vq_wav2vec_kmeans<a class="headerlink" href="#vq-wav2vec-kmeans" title="Permalink to this heading">#</a></h3>
<p>This is the official vq-wav2vec model from fairseq.
This model uses K-means as the quantization technique</p>
</section>
</section>
<section id="discrete-bert">
<h2>Discrete BERT<a class="headerlink" href="#discrete-bert" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1910.05453">vq-wav2vec: Self-supervised learning of discrete speech representations</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>baevski2019vq,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>vq-wav2vec:<span class="w"> </span>Self-Supervised<span class="w"> </span>Learning<span class="w"> </span>of<span class="w"> </span>Discrete<span class="w"> </span>Speech<span class="w"> </span>Representations<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Baevski,<span class="w"> </span>Alexei<span class="w"> </span>and<span class="w"> </span>Schneider,<span class="w"> </span>Steffen<span class="w"> </span>and<span class="w"> </span>Auli,<span class="w"> </span>Michael<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Learning<span class="w"> </span>Representations<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2019</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>This method takes the Conv feature encoder’s output, quantize it into token ids, and feed the
tokens into a NLP BERT (Specifically, RoBERTa). The output hidden_states are all the hidden hidden_states
of the NLP BERT (excluding the hidden_states in <a class="reference internal" href="#vq-wav2vec">vq-wav2vec</a>)</p>
<section id="discretebert">
<h3>discretebert<a class="headerlink" href="#discretebert" title="Permalink to this heading">#</a></h3>
<p>Alias of <a class="reference internal" href="#vq-wav2vec-kmeans-roberta">vq_wav2vec_kmeans_roberta</a></p>
</section>
<section id="vq-wav2vec-kmeans-roberta">
<h3>vq_wav2vec_kmeans_roberta<a class="headerlink" href="#vq-wav2vec-kmeans-roberta" title="Permalink to this heading">#</a></h3>
<p>This model uses <a class="reference internal" href="#vq-wav2vec-kmeans">vq_wav2vec_kmeans</a> as the frontend waveform tokenizer. After the waveform is tokenized
into a sequence of token ids, tokens are then fed into a RoBERTa model.</p>
</section>
</section>
<section id="wav2vec-2-0">
<h2>wav2vec 2.0<a class="headerlink" href="#wav2vec-2-0" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>baevski2020wav2vec,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>wav2vec<span class="w"> </span><span class="m">2</span>.0:<span class="w"> </span>A<span class="w"> </span>framework<span class="w"> </span><span class="k">for</span><span class="w"> </span>self-supervised<span class="w"> </span>learning<span class="w"> </span>of<span class="w"> </span>speech<span class="w"> </span>representations<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Baevski,<span class="w"> </span>Alexei<span class="w"> </span>and<span class="w"> </span>Zhou,<span class="w"> </span>Yuhao<span class="w"> </span>and<span class="w"> </span>Mohamed,<span class="w"> </span>Abdelrahman<span class="w"> </span>and<span class="w"> </span>Auli,<span class="w"> </span>Michael<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>Advances<span class="w"> </span><span class="k">in</span><span class="w"> </span>Neural<span class="w"> </span>Information<span class="w"> </span>Processing<span class="w"> </span>Systems<span class="o">}</span>,
<span class="w">    </span><span class="nv">volume</span><span class="o">={</span><span class="m">33</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">12449</span>--12460<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>All the entries below support the following <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>feature_selection</p></td>
<td><dl class="simple">
<dt>(str) -</dt><dd><p>if <code class="code docutils literal notranslate"><span class="pre">fairseq_layers</span></code> or <code class="code docutils literal notranslate"><span class="pre">fairseq_layers_before_residual</span></code>,
extract the representation following official fairseq API.
for <code class="code docutils literal notranslate"><span class="pre">fairseq_layers</span></code>, it is the output of each transformer
encoder layer; for <code class="code docutils literal notranslate"><span class="pre">fairseq_layers_before_residual</span></code>, it is
the output of the feedforward layer (before adding with the
main residual) of each transformer encoder layer. by default
this option is None, which follows the default place to extract
in S3PRL.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
<section id="wav2vec2-custom">
<h3>wav2vec2_custom<a class="headerlink" href="#wav2vec2-custom" title="Permalink to this heading">#</a></h3>
<p>This entry expects you to provide the source of the checkpoint: <code class="code docutils literal notranslate"><span class="pre">path_or_url</span></code>, which should be
the local path or a url of the checkpoint converted by <code class="code docutils literal notranslate"><span class="pre">s3prl/upstream/wav2vec2/convert.py</span></code> (
from a regular fairseq checkpoint.)</p>
<p>This entry also supports the following additional <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>fairseq</p></td>
<td><dl class="simple">
<dt>(bool) -</dt><dd><p>If True, perform the on-the-fly checkpoint conversion, so that
you can directly give the fairseq checkpoint to the <code class="code docutils literal notranslate"><span class="pre">path_or_url</span></code>
argument, either a fairseq URL or a fairseq checkpoint local path.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="hf-wav2vec2-custom">
<h3>hf_wav2vec2_custom<a class="headerlink" href="#hf-wav2vec2-custom" title="Permalink to this heading">#</a></h3>
<p>This entry expects you to provide the source of the checkpoint: <code class="code docutils literal notranslate"><span class="pre">path_or_url</span></code>, which should be
in the HuggingFace format, like <code class="code docutils literal notranslate"><span class="pre">facebook/wav2vec2-large-960h</span></code></p>
</section>
<section id="wav2vec2">
<h3>wav2vec2<a class="headerlink" href="#wav2vec2" title="Permalink to this heading">#</a></h3>
<p>This is the alias of <a class="reference internal" href="#wav2vec2-base-960">wav2vec2_base_960</a></p>
</section>
<section id="wav2vec2-base-960">
<h3>wav2vec2_base_960<a class="headerlink" href="#wav2vec2-base-960" title="Permalink to this heading">#</a></h3>
<p>This is the official wav2vec 2.0 model in fairseq</p>
<ul class="simple">
<li><p>Architecture: 12-layer Transformer encoders</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
<section id="wav2vec2-large-960">
<h3>wav2vec2_large_960<a class="headerlink" href="#wav2vec2-large-960" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Architecture: 24-layer Transformer encoders</p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
<section id="wav2vec2-large-ll60k">
<h3>wav2vec2_large_ll60k<a class="headerlink" href="#wav2vec2-large-ll60k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Architecture: 24-layer Transformer encoders</p></li>
<li><p>Unlabled Speech: LibriLight LL60k hours</p></li>
</ul>
</section>
<section id="wav2vec2-large-lv60-cv-swbd-fsh">
<h3>wav2vec2_large_lv60_cv_swbd_fsh<a class="headerlink" href="#wav2vec2-large-lv60-cv-swbd-fsh" title="Permalink to this heading">#</a></h3>
<p>The Large model trained on Libri-Light 60k hours + CommonVoice + Switchboard + Fisher</p>
<ul class="simple">
<li><p>Architecture: 24-layer Transformer encoders</p></li>
<li><p>Unlabeled Speech: Libri-Light 60k hours + CommonVoice + Switchboard + Fisher</p></li>
</ul>
</section>
<section id="wav2vec2-conformer-relpos">
<h3>wav2vec2_conformer_relpos<a class="headerlink" href="#wav2vec2-conformer-relpos" title="Permalink to this heading">#</a></h3>
<p>The results can be found in the Table 4 of <a class="reference external" href="https://arxiv.org/abs/2010.05171">fairseq S2T: Fast Speech-to-Text Modeling with fairseq</a>.</p>
<ul class="simple">
<li><p>Architecture: 24-layer Conformer encoders with relative positional encoding</p></li>
<li><p>Unlabeled Speech: LibriLight LL60k hours</p></li>
</ul>
</section>
<section id="wav2vec2-conformer-rope">
<h3>wav2vec2_conformer_rope<a class="headerlink" href="#wav2vec2-conformer-rope" title="Permalink to this heading">#</a></h3>
<p>The results can be found in the Table 4 of <a class="reference external" href="https://arxiv.org/abs/2010.05171">fairseq S2T: Fast Speech-to-Text Modeling with fairseq</a>.</p>
<ul class="simple">
<li><p>Architecture: 24-layer Conformer encoders with ROPE positional encoding</p></li>
<li><p>Unlabeled Speech: LibriLight LL60k hours</p></li>
</ul>
</section>
<section id="wav2vec2-base-s2st-es-voxpopuli">
<h3>wav2vec2_base_s2st_es_voxpopuli<a class="headerlink" href="#wav2vec2-base-s2st-es-voxpopuli" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The wav2vec2 model from <a class="reference external" href="https://arxiv.org/abs/2204.02967">Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation</a>,</p></li>
<li><p>released in Fairseq with the link: <a class="reference external" href="https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/es/transformer_B.pt">https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/es/transformer_B.pt</a></p></li>
</ul>
</section>
<section id="wav2vec2-base-s2st-en-librilight">
<h3>wav2vec2_base_s2st_en_librilight<a class="headerlink" href="#wav2vec2-base-s2st-en-librilight" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The wav2vec2 model from <a class="reference external" href="https://arxiv.org/abs/2204.02967">Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation</a>,</p></li>
<li><p>released in Fairseq with the link: <a class="reference external" href="https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/en/transformer_B.pt">https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/en/transformer_B.pt</a></p></li>
</ul>
</section>
<section id="wav2vec2-conformer-large-s2st-es-voxpopuli">
<h3>wav2vec2_conformer_large_s2st_es_voxpopuli<a class="headerlink" href="#wav2vec2-conformer-large-s2st-es-voxpopuli" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The wav2vec2 model from <a class="reference external" href="https://arxiv.org/abs/2204.02967">Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation</a>,</p></li>
<li><p>released in Fairseq with the link: <a class="reference external" href="https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/es/conformer_L.pt">https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/es/conformer_L.pt</a></p></li>
</ul>
</section>
<section id="wav2vec2-conformer-large-s2st-en-librilight">
<h3>wav2vec2_conformer_large_s2st_en_librilight<a class="headerlink" href="#wav2vec2-conformer-large-s2st-en-librilight" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The wav2vec2 model from <a class="reference external" href="https://arxiv.org/abs/2204.02967">Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation</a>,</p></li>
<li><p>released in Fairseq with the link: <a class="reference external" href="https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/en/conformer_L.pt">https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/s2st_finetuning/w2v2/en/conformer_L.pt</a></p></li>
</ul>
</section>
<section id="xlsr-53">
<h3>xlsr_53<a class="headerlink" href="#xlsr-53" title="Permalink to this heading">#</a></h3>
<p>The wav2vec 2.0 model trained on multilingual presented in <a class="reference external" href="https://arxiv.org/abs/2006.13979">Unsupervised Cross-lingual Representation Learning for Speech Recognition</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>conneau2020unsupervised,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Unsupervised<span class="w"> </span>cross-lingual<span class="w"> </span>representation<span class="w"> </span>learning<span class="w"> </span><span class="k">for</span><span class="w"> </span>speech<span class="w"> </span>recognition<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Conneau,<span class="w"> </span>Alexis<span class="w"> </span>and<span class="w"> </span>Baevski,<span class="w"> </span>Alexei<span class="w"> </span>and<span class="w"> </span>Collobert,<span class="w"> </span>Ronan<span class="w"> </span>and<span class="w"> </span>Mohamed,<span class="w"> </span>Abdelrahman<span class="w"> </span>and<span class="w"> </span>Auli,<span class="w"> </span>Michael<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2006.13979<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2020</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
</section>
</section>
<section id="xls-r">
<h2>XLS-R<a class="headerlink" href="#xls-r" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2111.09296">XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>babu2021xls,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>XLS-R:<span class="w"> </span>Self-supervised<span class="w"> </span>cross-lingual<span class="w"> </span>speech<span class="w"> </span>representation<span class="w"> </span>learning<span class="w"> </span>at<span class="w"> </span>scale<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Babu,<span class="w"> </span>Arun<span class="w"> </span>and<span class="w"> </span>Wang,<span class="w"> </span>Changhan<span class="w"> </span>and<span class="w"> </span>Tjandra,<span class="w"> </span>Andros<span class="w"> </span>and<span class="w"> </span>Lakhotia,<span class="w"> </span>Kushal<span class="w"> </span>and<span class="w"> </span>Xu,<span class="w"> </span>Qiantong<span class="w"> </span>and<span class="w"> </span>Goyal,<span class="w"> </span>Naman<span class="w"> </span>and<span class="w"> </span>Singh,<span class="w"> </span>Kritika<span class="w"> </span>and<span class="w"> </span>von<span class="w"> </span>Platen,<span class="w"> </span>Patrick<span class="w"> </span>and<span class="w"> </span>Saraf,<span class="w"> </span>Yatharth<span class="w"> </span>and<span class="w"> </span>Pino,<span class="w"> </span>Juan<span class="w"> </span>and<span class="w"> </span>others<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2111.09296<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="xls-r-300m">
<h3>xls_r_300m<a class="headerlink" href="#xls-r-300m" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: 128 languages, 436K hours</p></li>
</ul>
</section>
<section id="xls-r-1b">
<h3>xls_r_1b<a class="headerlink" href="#xls-r-1b" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: 128 languages, 436K hours</p></li>
</ul>
</section>
<section id="xls-r-2b">
<h3>xls_r_2b<a class="headerlink" href="#xls-r-2b" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: 128 languages, 436K hours</p></li>
</ul>
</section>
</section>
<section id="hubert">
<h2>HuBERT<a class="headerlink" href="#hubert" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2106.07447">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>hsu2021hubert,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Hubert:<span class="w"> </span>Self-supervised<span class="w"> </span>speech<span class="w"> </span>representation<span class="w"> </span>learning<span class="w"> </span>by<span class="w"> </span>masked<span class="w"> </span>prediction<span class="w"> </span>of<span class="w"> </span>hidden<span class="w"> </span>units<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Hsu,<span class="w"> </span>Wei-Ning<span class="w"> </span>and<span class="w"> </span>Bolte,<span class="w"> </span>Benjamin<span class="w"> </span>and<span class="w"> </span>Tsai,<span class="w"> </span>Yao-Hung<span class="w"> </span>Hubert<span class="w"> </span>and<span class="w"> </span>Lakhotia,<span class="w"> </span>Kushal<span class="w"> </span>and<span class="w"> </span>Salakhutdinov,<span class="w"> </span>Ruslan<span class="w"> </span>and<span class="w"> </span>Mohamed,<span class="w"> </span>Abdelrahman<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>IEEE/ACM<span class="w"> </span>Transactions<span class="w"> </span>on<span class="w"> </span>Audio,<span class="w"> </span>Speech,<span class="w"> </span>and<span class="w"> </span>Language<span class="w"> </span>Processing<span class="o">}</span>,
<span class="w">    </span><span class="nv">volume</span><span class="o">={</span><span class="m">29</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">3451</span>--3460<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">publisher</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="hubert-custom">
<h3>hubert_custom<a class="headerlink" href="#hubert-custom" title="Permalink to this heading">#</a></h3>
<p>This entry expects you to provide the source of the checkpoint: <code class="code docutils literal notranslate"><span class="pre">path_or_url</span></code>, which should be
the local path or a url of the checkpoint converted by <code class="code docutils literal notranslate"><span class="pre">s3prl/upstream/hubert/convert.py</span></code> (
from a regular fairseq checkpoint.)</p>
<p>This entry also supports the following additional <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>.</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>fairseq</p></td>
<td><dl class="simple">
<dt>(bool) -</dt><dd><p>If True, perform the on-the-fly checkpoint conversion, so that
you can directly give the fairseq checkpoint to the <code class="code docutils literal notranslate"><span class="pre">path_or_url</span></code>
argument, either a fairseq URL or a fairseq checkpoint local path.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="hf-hubert-custom">
<h3>hf_hubert_custom<a class="headerlink" href="#hf-hubert-custom" title="Permalink to this heading">#</a></h3>
<p>This entry expects you to provide the source of the checkpoint: <code class="code docutils literal notranslate"><span class="pre">path_or_url</span></code>, which should be
in the HuggingFace format, like <code class="code docutils literal notranslate"><span class="pre">facebook/hubert-large-ll60k</span></code></p>
</section>
<section id="id15">
<h3>hubert<a class="headerlink" href="#id15" title="Permalink to this heading">#</a></h3>
<p>This is alias of <a class="reference internal" href="#hubert-base">hubert_base</a></p>
</section>
<section id="hubert-base">
<h3>hubert_base<a class="headerlink" href="#hubert-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
<section id="hubert-large-ll60k">
<h3>hubert_large_ll60k<a class="headerlink" href="#hubert-large-ll60k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriLight ll60k hours</p></li>
</ul>
</section>
<section id="mhubert-base-vp-en-es-fr-it3">
<h3>mhubert_base_vp_en_es_fr_it3<a class="headerlink" href="#mhubert-base-vp-en-es-fr-it3" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The multilingual model from <a class="reference external" href="https://arxiv.org/abs/2112.08352">Textless Speech-to-Speech Translation on Real Data</a></p></li>
</ul>
</section>
</section>
<section id="espnethubert">
<h2>ESPnetHuBERT<a class="headerlink" href="#espnethubert" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2306.06672">Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chen23l_interspeech,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>William<span class="w"> </span>Chen<span class="w"> </span>and<span class="w"> </span>Xuankai<span class="w"> </span>Chang<span class="w"> </span>and<span class="w"> </span>Yifan<span class="w"> </span>Peng<span class="w"> </span>and<span class="w"> </span>Zhaoheng<span class="w"> </span>Ni<span class="w"> </span>and<span class="w"> </span>Soumi<span class="w"> </span>Maiti<span class="w"> </span>and<span class="w"> </span>Shinji<span class="w"> </span>Watanabe<span class="o">}</span>,
<span class="w">    </span><span class="nv">title</span><span class="o">={{</span>Reducing<span class="w"> </span>Barriers<span class="w"> </span>to<span class="w"> </span>Self-Supervised<span class="w"> </span>Learning:<span class="w"> </span>HuBERT<span class="w"> </span>Pre-training<span class="w"> </span>with<span class="w"> </span>Academic<span class="w"> </span>Compute<span class="o">}}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">=</span><span class="m">2023</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>Proc.<span class="w"> </span>INTERSPEECH<span class="w"> </span><span class="m">2023</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">4404</span>--4408<span class="o">}</span>,
<span class="w">    </span><span class="nv">doi</span><span class="o">={</span><span class="m">10</span>.21437/Interspeech.2023-1176<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="espnet-hubert-custom">
<h3>espnet_hubert_custom<a class="headerlink" href="#espnet-hubert-custom" title="Permalink to this heading">#</a></h3>
<p>This entry expects you to provide the source of the checkpoint: <code class="code docutils literal notranslate"><span class="pre">ckpt</span></code>, which should be
the local path of the checkpoint pretrained from ESPnet (e.g., latest.pth).</p>
</section>
<section id="espnet-hubert-base-iter0">
<h3>espnet_hubert_base_iter0<a class="headerlink" href="#espnet-hubert-base-iter0" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: LibriSpeech 960hr (first iteration of HuBERT pre-training)</p></li>
</ul>
</section>
<section id="espnet-hubert-base-iter1">
<h3>espnet_hubert_base_iter1<a class="headerlink" href="#espnet-hubert-base-iter1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: LibriSpeech 960hr (second iteration of HuBERT pre-training)</p></li>
</ul>
</section>
<section id="espnet-hubert-large-gs-ll60k">
<h3>espnet_hubert_large_gs_ll60k<a class="headerlink" href="#espnet-hubert-large-gs-ll60k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: LibriLight ll60k hours</p></li>
<li><p>Labeled Speech: GigaSpeech 10k hours (to get units)</p></li>
</ul>
</section>
</section>
<section id="wavlablm">
<h2>WavLabLM<a class="headerlink" href="#wavlablm" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2309.15317">Joint Prediction and Denoising for Large-scale Multilingual Self-supervised Learning</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chen23joint,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>William<span class="w"> </span>Chen<span class="w"> </span>and<span class="w"> </span>Jiatong<span class="w"> </span>Shi<span class="w"> </span>and<span class="w"> </span>Brian<span class="w"> </span>Yan<span class="w"> </span>and<span class="w"> </span>Dan<span class="w"> </span>Berrebbi<span class="w"> </span>and<span class="w"> </span>Wangyou<span class="w"> </span>Zhang<span class="w"> </span>and<span class="w"> </span>Yifan<span class="w"> </span>Peng<span class="w"> </span>and<span class="w"> </span>Xuankai<span class="w"> </span>Chang<span class="w"> </span>and<span class="w"> </span>Soumi<span class="w"> </span>Maiti<span class="w"> </span>and<span class="w"> </span>Shinji<span class="w"> </span>Watanabe<span class="o">}</span>,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Joint<span class="w"> </span>Prediction<span class="w"> </span>and<span class="w"> </span>Denoising<span class="w"> </span><span class="k">for</span><span class="w"> </span>Large-scale<span class="w"> </span>Multilingual<span class="w"> </span>Self-supervised<span class="w"> </span>Learning<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">=</span><span class="m">2023</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>IEEE<span class="w"> </span>Automatic<span class="w"> </span>Speech<span class="w"> </span>Recognition<span class="w"> </span>and<span class="w"> </span>Understanding<span class="w"> </span>Workshop<span class="w"> </span><span class="o">(</span>ASRU<span class="o">)}</span>,
<span class="o">}</span>
</pre></div>
</div>
<section id="cvhubert">
<h3>cvhubert<a class="headerlink" href="#cvhubert" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Commonvoice V11 Multilingual Data (13.6k hours)</p></li>
<li><p>only 20ms resolution version is provided. <a class="reference external" href="https://huggingface.co/espnet/espnet_cvhubert/tree/main">check huggingface  for other resolutions</a></p></li>
</ul>
</section>
<section id="wavlablm-ek-40k">
<h3>wavlablm_ek_40k<a class="headerlink" href="#wavlablm-ek-40k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Openli110 (Combination of Commonvoice, Voxpopuli, MLS, Googlei18n, around 39k hours)</p></li>
<li><p>Initialed from hubert_large_ll60k and continue train with English based k-means from librispeech</p></li>
</ul>
</section>
<section id="wavlablm-mk-40k">
<h3>wavlablm_mk_40k<a class="headerlink" href="#wavlablm-mk-40k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Openli110 (Combination of Commonvoice, Voxpopuli, MLS, Googlei18n, around 39k hours)</p></li>
<li><p>Trained from scratch and use a multilingual k-means from the training data</p></li>
</ul>
</section>
<section id="wavlablm-ms-40k">
<h3>wavlablm_ms_40k<a class="headerlink" href="#wavlablm-ms-40k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Openli110 (Combination of Commonvoice, Voxpopuli, MLS, Googlei18n, around 39k hours)</p></li>
<li><p>Trained from scratch and use a multilingual k-means from the training data with a multi-stage training</p></li>
</ul>
</section>
</section>
<section id="multiresolution-hubert-mr-hubert">
<h2>Multiresolution HuBERT (MR-HuBERT)<a class="headerlink" href="#multiresolution-hubert-mr-hubert" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://openreview.net/pdf?id=kUuKFW7DIF">Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>anonymous2023multiresolution,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Multi-resolution<span class="w"> </span>Hu<span class="o">{</span>BERT<span class="o">}</span>:<span class="w"> </span>Multi-resolution<span class="w"> </span>Speech<span class="w"> </span>Self-Supervised<span class="w"> </span>Learning<span class="w"> </span>with<span class="w"> </span>Masked<span class="w"> </span>Unit<span class="w"> </span>Prediction<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Anonymous<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>Submitted<span class="w"> </span>to<span class="w"> </span>The<span class="w"> </span>Twelfth<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Learning<span class="w"> </span>Representations<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2023</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">url</span><span class="o">={</span>https://openreview.net/forum?id<span class="o">=</span>kUuKFW7DIF<span class="o">}</span>,
<span class="w">    </span><span class="nv">note</span><span class="o">={</span>under<span class="w"> </span>review<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="multires-hubert-custom">
<h3>multires_hubert_custom<a class="headerlink" href="#multires-hubert-custom" title="Permalink to this heading">#</a></h3>
<p>This entry expects you to provide the source of the checkpoint: <code class="code docutils literal notranslate"><span class="pre">ckpt</span></code>, which should be
the local path or a url of the checkpoint converted by <code class="code docutils literal notranslate"><span class="pre">s3prl/upstream/multires_hubert/convert.py</span></code> (
from a regular fairseq checkpoint.)
For more available checkpoints, please check <a class="reference external" href="https://github.com/facebookresearch/fairseq/blob/main/examples/mr_hubert/README.md">Fairseq official release</a>
Related converted checkpoints are also at <a class="reference external" href="https://huggingface.co/s3prl/mr_hubert">S3PRL HuggingFace Repo</a></p>
</section>
<section id="multires-hubert-base">
<h3>multires_hubert_base<a class="headerlink" href="#multires-hubert-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
<li><p>K-means extracted from <a class="reference internal" href="#hubert-base">hubert_base</a></p></li>
</ul>
</section>
<section id="multires-hubert-large">
<h3>multires_hubert_large<a class="headerlink" href="#multires-hubert-large" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: LibriLight 60khr</p></li>
<li><p>K-means extracted from <a class="reference internal" href="#hubert-base">hubert_base</a></p></li>
</ul>
</section>
<section id="multires-hubert-multilingual-base">
<h3>multires_hubert_multilingual_base<a class="headerlink" href="#multires-hubert-multilingual-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Voxpopuli 100khr</p></li>
<li><p>K-means extracted from <a class="reference internal" href="#hubert-base">hubert_base</a></p></li>
</ul>
</section>
<section id="multires-hubert-multilingual-large400k">
<h3>multires_hubert_multilingual_large400k<a class="headerlink" href="#multires-hubert-multilingual-large400k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Voxpopuli 100khr</p></li>
<li><p>K-means extracted from <a class="reference internal" href="#hubert-base">hubert_base</a></p></li>
<li><p>Training steps 400k</p></li>
</ul>
</section>
<section id="multires-hubert-multilingual-large600k">
<h3>multires_hubert_multilingual_large600k<a class="headerlink" href="#multires-hubert-multilingual-large600k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabeled Speech: Voxpopuli 100khr</p></li>
<li><p>K-means extracted from <a class="reference internal" href="#hubert-base">hubert_base</a></p></li>
<li><p>Training steps 600k</p></li>
</ul>
</section>
</section>
<section id="distilhubert">
<h2>DistilHuBERT<a class="headerlink" href="#distilhubert" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2110.01900">DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chang2022distilhubert,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>DistilHuBERT:<span class="w"> </span>Speech<span class="w"> </span>representation<span class="w"> </span>learning<span class="w"> </span>by<span class="w"> </span>layer-wise<span class="w"> </span>distillation<span class="w"> </span>of<span class="w"> </span>hidden-unit<span class="w"> </span>BERT<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Chang,<span class="w"> </span>Heng-Jui<span class="w"> </span>and<span class="w"> </span>Yang,<span class="w"> </span>Shu-wen<span class="w"> </span>and<span class="w"> </span>Lee,<span class="w"> </span>Hung-yi<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>ICASSP<span class="w"> </span><span class="m">2022</span>-2022<span class="w"> </span>IEEE<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Acoustics,<span class="w"> </span>Speech<span class="w"> </span>and<span class="w"> </span>Signal<span class="w"> </span>Processing<span class="w"> </span><span class="o">(</span>ICASSP<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">7087</span>--7091<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id16">
<h3>distilhubert<a class="headerlink" href="#id16" title="Permalink to this heading">#</a></h3>
<p>Alias of <a class="reference internal" href="#distilhubert-base">distilhubert_base</a></p>
</section>
<section id="distilhubert-base">
<h3>distilhubert_base<a class="headerlink" href="#distilhubert-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Teacher: <a class="reference internal" href="#hubert-base">hubert_base</a></p></li>
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
</ul>
</section>
</section>
<section id="hubert-mgr">
<h2>HuBERT-MGR<a class="headerlink" href="#hubert-mgr" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2203.16104">Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>huang2022improving,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Improving<span class="w"> </span>Distortion<span class="w"> </span>Robustness<span class="w"> </span>of<span class="w"> </span>Self-supervised<span class="w"> </span>Speech<span class="w"> </span>Processing<span class="w"> </span>Tasks<span class="w"> </span>with<span class="w"> </span>Domain<span class="w"> </span>Adaptation<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Huang,<span class="w"> </span>Kuan<span class="w"> </span>Po<span class="w"> </span>and<span class="w"> </span>Fu,<span class="w"> </span>Yu-Kuan<span class="w"> </span>and<span class="w"> </span>Zhang,<span class="w"> </span>Yu<span class="w"> </span>and<span class="w"> </span>Lee,<span class="w"> </span>Hung-yi<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2203.16104<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="hubert-base-robust-mgr">
<h3>hubert_base_robust_mgr<a class="headerlink" href="#hubert-base-robust-mgr" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Speech: LibriSpeech 960hr</p></li>
<li><p>Augmentation: MUSAN, gaussian, reverberation</p></li>
</ul>
</section>
</section>
<section id="unispeech-sat">
<h2>Unispeech-SAT<a class="headerlink" href="#unispeech-sat" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2110.05752">Unispeech-sat: Universal speech representation learning with speaker aware pre-training</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>chen2022unispeech,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Unispeech-sat:<span class="w"> </span>Universal<span class="w"> </span>speech<span class="w"> </span>representation<span class="w"> </span>learning<span class="w"> </span>with<span class="w"> </span>speaker<span class="w"> </span>aware<span class="w"> </span>pre-training<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Chen,<span class="w"> </span>Sanyuan<span class="w"> </span>and<span class="w"> </span>Wu,<span class="w"> </span>Yu<span class="w"> </span>and<span class="w"> </span>Wang,<span class="w"> </span>Chengyi<span class="w"> </span>and<span class="w"> </span>Chen,<span class="w"> </span>Zhengyang<span class="w"> </span>and<span class="w"> </span>Chen,<span class="w"> </span>Zhuo<span class="w"> </span>and<span class="w"> </span>Liu,<span class="w"> </span>Shujie<span class="w"> </span>and<span class="w"> </span>Wu,<span class="w"> </span>Jian<span class="w"> </span>and<span class="w"> </span>Qian,<span class="w"> </span>Yao<span class="w"> </span>and<span class="w"> </span>Wei,<span class="w"> </span>Furu<span class="w"> </span>and<span class="w"> </span>Li,<span class="w"> </span>Jinyu<span class="w"> </span>and<span class="w"> </span>others<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>ICASSP<span class="w"> </span><span class="m">2022</span>-2022<span class="w"> </span>IEEE<span class="w"> </span>International<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Acoustics,<span class="w"> </span>Speech<span class="w"> </span>and<span class="w"> </span>Signal<span class="w"> </span>Processing<span class="w"> </span><span class="o">(</span>ICASSP<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">6152</span>--6156<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id17">
<h3>unispeech_sat<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h3>
<p>Alias of <a class="reference internal" href="#unispeech-sat-base">unispeech_sat_base</a></p>
</section>
<section id="unispeech-sat-base">
<h3>unispeech_sat_base<a class="headerlink" href="#unispeech-sat-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 12 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriSpeech 960 hours</p></li>
</ul>
</section>
<section id="unispeech-sat-base-plus">
<h3>unispeech_sat_base_plus<a class="headerlink" href="#unispeech-sat-base-plus" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 12 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriLight 60k hours + Gigaspeech 10k hours + VoxPopuli 24k hours = 94k hours</p></li>
</ul>
</section>
<section id="unispeech-sat-large">
<h3>unispeech_sat_large<a class="headerlink" href="#unispeech-sat-large" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 24 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriLight 60k hours + Gigaspeech 10k hours + VoxPopuli 24k hours = 94k hours</p></li>
</ul>
</section>
</section>
<section id="wavlm">
<h2>WavLM<a class="headerlink" href="#wavlm" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2110.13900">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>Chen2021WavLM,
<span class="w">    </span><span class="nv">title</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>WavLM:<span class="w"> </span>Large-Scale<span class="w"> </span>Self-Supervised<span class="w">  </span>Pre-training<span class="w">   </span><span class="k">for</span><span class="w"> </span>Full<span class="w"> </span>Stack<span class="w"> </span>Speech<span class="w"> </span>Processing<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="o">{</span>Sanyuan<span class="w"> </span>Chen<span class="w"> </span>and<span class="w"> </span>Chengyi<span class="w"> </span>Wang<span class="w"> </span>and<span class="w"> </span>Zhengyang<span class="w"> </span>Chen<span class="w"> </span>and<span class="w"> </span>Yu<span class="w"> </span>Wu<span class="w"> </span>and<span class="w"> </span>Shujie<span class="w"> </span>Liu<span class="w"> </span>and<span class="w"> </span>Zhuo<span class="w"> </span>Chen<span class="w"> </span>and<span class="w"> </span>Jinyu<span class="w"> </span>Li<span class="w"> </span>and<span class="w"> </span>Naoyuki<span class="w"> </span>Kanda<span class="w"> </span>and<span class="w"> </span>Takuya<span class="w"> </span>Yoshioka<span class="w"> </span>and<span class="w"> </span>Xiong<span class="w"> </span>Xiao<span class="w"> </span>and<span class="w"> </span>Jian<span class="w"> </span>Wu<span class="w"> </span>and<span class="w"> </span>Long<span class="w"> </span>Zhou<span class="w"> </span>and<span class="w"> </span>Shuo<span class="w"> </span>Ren<span class="w"> </span>and<span class="w"> </span>Yanmin<span class="w"> </span>Qian<span class="w"> </span>and<span class="w"> </span>Yao<span class="w"> </span>Qian<span class="w"> </span>and<span class="w"> </span>Jian<span class="w"> </span>Wu<span class="w"> </span>and<span class="w"> </span>Michael<span class="w"> </span>Zeng<span class="w"> </span>and<span class="w"> </span>Furu<span class="w"> </span>Wei<span class="o">}</span>,
<span class="w">    </span><span class="nv">eprint</span><span class="o">={</span><span class="m">2110</span>.13900<span class="o">}</span>,
<span class="w">    </span><span class="nv">archivePrefix</span><span class="o">={</span>arXiv<span class="o">}</span>,
<span class="w">    </span><span class="nv">primaryClass</span><span class="o">={</span>cs.CL<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id18">
<h3>wavlm<a class="headerlink" href="#id18" title="Permalink to this heading">#</a></h3>
<p>Alias of <a class="reference internal" href="#wavlm-base-plus">wavlm_base_plus</a></p>
</section>
<section id="wavlm-base">
<h3>wavlm_base<a class="headerlink" href="#wavlm-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 12 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriSpeech 960 hours</p></li>
</ul>
</section>
<section id="wavlm-base-plus">
<h3>wavlm_base_plus<a class="headerlink" href="#wavlm-base-plus" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 12 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriLight 60k hours + Gigaspeech 10k hours + VoxPopuli 24k hours = 94k hours</p></li>
</ul>
</section>
<section id="wavlm-large">
<h3>wavlm_large<a class="headerlink" href="#wavlm-large" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 24 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriLight 60k hours + Gigaspeech 10k hours + VoxPopuli 24k hours = 94k hours</p></li>
</ul>
</section>
</section>
<section id="data2vec">
<h2>data2vec<a class="headerlink" href="#data2vec" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2202.03555">data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>baevski2022data2vec,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Data2vec:<span class="w"> </span>A<span class="w"> </span>general<span class="w"> </span>framework<span class="w"> </span><span class="k">for</span><span class="w"> </span>self-supervised<span class="w"> </span>learning<span class="w"> </span><span class="k">in</span><span class="w"> </span>speech,<span class="w"> </span>vision<span class="w"> </span>and<span class="w"> </span>language<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Baevski,<span class="w"> </span>Alexei<span class="w"> </span>and<span class="w"> </span>Hsu,<span class="w"> </span>Wei-Ning<span class="w"> </span>and<span class="w"> </span>Xu,<span class="w"> </span>Qiantong<span class="w"> </span>and<span class="w"> </span>Babu,<span class="w"> </span>Arun<span class="w"> </span>and<span class="w"> </span>Gu,<span class="w"> </span>Jiatao<span class="w"> </span>and<span class="w"> </span>Auli,<span class="w"> </span>Michael<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2202.03555<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id19">
<h3>data2vec<a class="headerlink" href="#id19" title="Permalink to this heading">#</a></h3>
<p>Alias of <a class="reference internal" href="#data2vec-base-960">data2vec_base_960</a></p>
</section>
<section id="data2vec-base-960">
<h3>data2vec_base_960<a class="headerlink" href="#data2vec-base-960" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 12 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriSpeech 960 hours</p></li>
</ul>
</section>
<section id="data2vec-large-ll60k">
<h3>data2vec_large_ll60k<a class="headerlink" href="#data2vec-large-ll60k" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Model Architecture: 24 layers Transformer blocks</p></li>
<li><p>Unlabled Speech: LibriLight 60k hours</p></li>
</ul>
</section>
</section>
<section id="ast">
<h2>AST<a class="headerlink" href="#ast" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2104.01778">AST: Audio Spectrogram Transformer</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>gong2021ast,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Ast:<span class="w"> </span>Audio<span class="w"> </span>spectrogram<span class="w"> </span>transformer<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Gong,<span class="w"> </span>Yuan<span class="w"> </span>and<span class="w"> </span>Chung,<span class="w"> </span>Yu-An<span class="w"> </span>and<span class="w"> </span>Glass,<span class="w"> </span>James<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2104.01778<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>All the entries below support the following <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>window_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>The segment waveform length to feed into the
AST model. If the input waveform is longer than this
length, do sliding windowing on the waveform and concat
the results along the time axis.</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>stride_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>When doing sliding window on the waveform (see
above), the stride seconds between windows.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
<section id="id20">
<h3>ast<a class="headerlink" href="#id20" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Labeled Data: AudioSet</p></li>
</ul>
</section>
</section>
<section id="ssast">
<h2>SSAST<a class="headerlink" href="#ssast" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2110.09784">SSAST: Self-Supervised Audio Spectrogram Transformer</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>gong2022ssast,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Ssast:<span class="w"> </span>Self-supervised<span class="w"> </span>audio<span class="w"> </span>spectrogram<span class="w"> </span>transformer<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Gong,<span class="w"> </span>Yuan<span class="w"> </span>and<span class="w"> </span>Lai,<span class="w"> </span>Cheng-I<span class="w"> </span>and<span class="w"> </span>Chung,<span class="w"> </span>Yu-An<span class="w"> </span>and<span class="w"> </span>Glass,<span class="w"> </span>James<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span>Proceedings<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>AAAI<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Artificial<span class="w"> </span>Intelligence<span class="o">}</span>,
<span class="w">    </span><span class="nv">volume</span><span class="o">={</span><span class="m">36</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">number</span><span class="o">={</span><span class="m">10</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">10699</span>--10709<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>All the entries below support the following <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>window_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>The segment waveform length to feed into the
AST model. If the input waveform is longer than this
length, do sliding windowing on the waveform and concat
the results along the time axis.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
<section id="ssast-frame-base">
<h3>ssast_frame_base<a class="headerlink" href="#ssast-frame-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: LibriSpeech &amp; AudioSet</p></li>
<li><p>fbank patch size: 128 (freq) * 2 (time)</p></li>
</ul>
</section>
<section id="ssast-patch-base">
<h3>ssast_patch_base<a class="headerlink" href="#ssast-patch-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: LibriSpeech &amp; AudioSet</p></li>
<li><p>fbank patch size: 16 (freq) * 16 (time)</p></li>
</ul>
</section>
</section>
<section id="mae-ast">
<h2>MAE-AST<a class="headerlink" href="#mae-ast" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2203.16691">MAE-AST: Masked Autoencoding Audio Spectrogram Transformer</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>baade2022mae,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>MAE-AST:<span class="w"> </span>Masked<span class="w"> </span>Autoencoding<span class="w"> </span>Audio<span class="w"> </span>Spectrogram<span class="w"> </span>Transformer<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Baade,<span class="w"> </span>Alan<span class="w"> </span>and<span class="w"> </span>Peng,<span class="w"> </span>Puyuan<span class="w"> </span>and<span class="w"> </span>Harwath,<span class="w"> </span>David<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2203.16691<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="mae-ast-frame">
<h3>mae_ast_frame<a class="headerlink" href="#mae-ast-frame" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: LibriSpeech &amp; AudioSet</p></li>
<li><p>fbank patch size: 128 (freq) * 2 (time)</p></li>
</ul>
</section>
<section id="mae-ast-patch">
<h3>mae_ast_patch<a class="headerlink" href="#mae-ast-patch" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: LibriSpeech &amp; AudioSet</p></li>
<li><p>fbank patch size: 16 (freq) * 16 (time)</p></li>
</ul>
</section>
</section>
<section id="byol-a">
<h2>Byol-A<a class="headerlink" href="#byol-a" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2103.06695">BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>niizumi2021byol,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>BYOL<span class="w"> </span><span class="k">for</span><span class="w"> </span>audio:<span class="w"> </span>Self-supervised<span class="w"> </span>learning<span class="w"> </span><span class="k">for</span><span class="w"> </span>general-purpose<span class="w"> </span>audio<span class="w"> </span>representation<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Niizumi,<span class="w"> </span>Daisuke<span class="w"> </span>and<span class="w"> </span>Takeuchi,<span class="w"> </span>Daiki<span class="w"> </span>and<span class="w"> </span>Ohishi,<span class="w"> </span>Yasunori<span class="w"> </span>and<span class="w"> </span>Harada,<span class="w"> </span>Noboru<span class="w"> </span>and<span class="w"> </span>Kashino,<span class="w"> </span>Kunio<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span><span class="m">2021</span><span class="w"> </span>International<span class="w"> </span>Joint<span class="w"> </span>Conference<span class="w"> </span>on<span class="w"> </span>Neural<span class="w"> </span>Networks<span class="w"> </span><span class="o">(</span>IJCNN<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">1</span>--8<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>All the entries below support the following <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>window_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>The segment waveform length to feed into the
AST model. If the input waveform is longer than this
length, do sliding windowing on the waveform and concat
the results along the time axis.</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>stride_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>When doing sliding window on the waveform (see
above), the stride seconds between windows.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
<section id="byol-a-2048">
<h3>byol_a_2048<a class="headerlink" href="#byol-a-2048" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: AudioSet</p></li>
</ul>
</section>
<section id="byol-a-1024">
<h3>byol_a_1024<a class="headerlink" href="#byol-a-1024" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: AudioSet</p></li>
</ul>
</section>
<section id="byol-a-512">
<h3>byol_a_512<a class="headerlink" href="#byol-a-512" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: AudioSet</p></li>
</ul>
</section>
</section>
<section id="byol-s">
<h2>Byol-S<a class="headerlink" href="#byol-s" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2206.12038">BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>elbanna2022byol,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Byol-s:<span class="w"> </span>Learning<span class="w"> </span>self-supervised<span class="w"> </span>speech<span class="w"> </span>representations<span class="w"> </span>by<span class="w"> </span>bootstrapping<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Elbanna,<span class="w"> </span>Gasser<span class="w"> </span>and<span class="w"> </span>Scheidwasser-Clow,<span class="w"> </span>Neil<span class="w"> </span>and<span class="w"> </span>Kegler,<span class="w"> </span>Mikolaj<span class="w"> </span>and<span class="w"> </span>Beckmann,<span class="w"> </span>Pierre<span class="w"> </span>and<span class="w"> </span>Hajal,<span class="w"> </span>Karl<span class="w"> </span>El<span class="w"> </span>and<span class="w"> </span>Cernak,<span class="w"> </span>Milos<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2206.12038<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2022</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="byol-s-default">
<h3>byol_s_default<a class="headerlink" href="#byol-s-default" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: AudioSet (Speech subset)</p></li>
</ul>
</section>
<section id="byol-s-cvt">
<h3>byol_s_cvt<a class="headerlink" href="#byol-s-cvt" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: AudioSet (Speech subset)</p></li>
</ul>
</section>
<section id="byol-s-resnetish34">
<h3>byol_s_resnetish34<a class="headerlink" href="#byol-s-resnetish34" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabled Data: AudioSet (Speech subset)</p></li>
</ul>
</section>
</section>
<section id="vggish">
<h2>VGGish<a class="headerlink" href="#vggish" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1609.09430">CNN Architectures for Large-Scale Audio Classification</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@inproceedings<span class="o">{</span>hershey2017cnn,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>CNN<span class="w"> </span>architectures<span class="w"> </span><span class="k">for</span><span class="w"> </span>large-scale<span class="w"> </span>audio<span class="w"> </span>classification<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Hershey,<span class="w"> </span>Shawn<span class="w"> </span>and<span class="w"> </span>Chaudhuri,<span class="w"> </span>Sourish<span class="w"> </span>and<span class="w"> </span>Ellis,<span class="w"> </span>Daniel<span class="w"> </span>PW<span class="w"> </span>and<span class="w"> </span>Gemmeke,<span class="w"> </span>Jort<span class="w"> </span>F<span class="w"> </span>and<span class="w"> </span>Jansen,<span class="w"> </span>Aren<span class="w"> </span>and<span class="w"> </span>Moore,<span class="w"> </span>R<span class="w"> </span>Channing<span class="w"> </span>and<span class="w"> </span>Plakal,<span class="w"> </span>Manoj<span class="w"> </span>and<span class="w"> </span>Platt,<span class="w"> </span>Devin<span class="w"> </span>and<span class="w"> </span>Saurous,<span class="w"> </span>Rif<span class="w"> </span>A<span class="w"> </span>and<span class="w"> </span>Seybold,<span class="w"> </span>Bryan<span class="w"> </span>and<span class="w"> </span>others<span class="o">}</span>,
<span class="w">    </span><span class="nv">booktitle</span><span class="o">={</span><span class="m">2017</span><span class="w"> </span>ieee<span class="w"> </span>international<span class="w"> </span>conference<span class="w"> </span>on<span class="w"> </span>acoustics,<span class="w"> </span>speech<span class="w"> </span>and<span class="w"> </span>signal<span class="w"> </span>processing<span class="w"> </span><span class="o">(</span>icassp<span class="o">)}</span>,
<span class="w">    </span><span class="nv">pages</span><span class="o">={</span><span class="m">131</span>--135<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2017</span><span class="o">}</span>,
<span class="w">    </span><span class="nv">organization</span><span class="o">={</span>IEEE<span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<section id="id21">
<h3>vggish<a class="headerlink" href="#id21" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Labaled Data: AudioSet</p></li>
</ul>
</section>
</section>
<section id="passt">
<h2>PaSST<a class="headerlink" href="#passt" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/2110.05069">Efficient Training of Audio Transformers with Patchout</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>@article<span class="o">{</span>koutini2021efficient,
<span class="w">    </span><span class="nv">title</span><span class="o">={</span>Efficient<span class="w"> </span>training<span class="w"> </span>of<span class="w"> </span>audio<span class="w"> </span>transformers<span class="w"> </span>with<span class="w"> </span>patchout<span class="o">}</span>,
<span class="w">    </span><span class="nv">author</span><span class="o">={</span>Koutini,<span class="w"> </span>Khaled<span class="w"> </span>and<span class="w"> </span>Schl<span class="o">{</span><span class="se">\&quot;</span>u<span class="o">}</span>ter,<span class="w"> </span>Jan<span class="w"> </span>and<span class="w"> </span>Eghbal-zadeh,<span class="w"> </span>Hamid<span class="w"> </span>and<span class="w"> </span>Widmer,<span class="w"> </span>Gerhard<span class="o">}</span>,
<span class="w">    </span><span class="nv">journal</span><span class="o">={</span>arXiv<span class="w"> </span>preprint<span class="w"> </span>arXiv:2110.05069<span class="o">}</span>,
<span class="w">    </span><span class="nv">year</span><span class="o">={</span><span class="m">2021</span><span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>All the entries below support the following <code class="code docutils literal notranslate"><span class="pre">extra_conf</span></code>:</p>
<div class="table-wrapper docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>column</p></th>
<th class="head"><p>description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>window_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>The segment waveform length to feed into the
model. If the input waveform is longer than this
length, do sliding windowing on the waveform and concat
the results along the time axis.</p>
</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td><p>stride_secs</p></td>
<td><dl class="simple">
<dt>(float) -</dt><dd><p>When doing sliding window on the waveform (see
above), the stride seconds between windows.</p>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</div>
<section id="passt-base">
<h3>passt_base<a class="headerlink" href="#passt-base" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Labaled Data: AudioSet</p></li>
</ul>
<p>Authors:</p>
<ul class="simple">
<li><p>Leo 2022</p></li>
</ul>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="problem.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Use Problem module to run customizable recipes</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="installation.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Install S3PRL</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022, S3PRL Team
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">S3PRL Upstream Collection</a><ul>
<li><a class="reference internal" href="#ssl-method">SSL Method</a></li>
<li><a class="reference internal" href="#mockingjay">Mockingjay</a></li>
<li><a class="reference internal" href="#tera">TERA</a></li>
<li><a class="reference internal" href="#audio-albert">Audio ALBERT</a></li>
<li><a class="reference internal" href="#apc">APC</a></li>
<li><a class="reference internal" href="#vq-apc">VQ-APC</a></li>
<li><a class="reference internal" href="#npc">NPC</a></li>
<li><a class="reference internal" href="#pase">PASE+</a></li>
<li><a class="reference internal" href="#modified-cpc">Modified CPC</a></li>
<li><a class="reference internal" href="#decoar">DeCoAR</a></li>
<li><a class="reference internal" href="#decoar-2-0">DeCoAR 2.0</a></li>
<li><a class="reference internal" href="#wav2vec">wav2vec</a></li>
<li><a class="reference internal" href="#vq-wav2vec">vq-wav2vec</a></li>
<li><a class="reference internal" href="#discrete-bert">Discrete BERT</a></li>
<li><a class="reference internal" href="#wav2vec-2-0">wav2vec 2.0</a></li>
<li><a class="reference internal" href="#xls-r">XLS-R</a></li>
<li><a class="reference internal" href="#hubert">HuBERT</a></li>
<li><a class="reference internal" href="#espnethubert">ESPnetHuBERT</a></li>
<li><a class="reference internal" href="#wavlablm">WavLabLM</a></li>
<li><a class="reference internal" href="#multiresolution-hubert-mr-hubert">Multiresolution HuBERT (MR-HuBERT)</a></li>
<li><a class="reference internal" href="#distilhubert">DistilHuBERT</a></li>
<li><a class="reference internal" href="#hubert-mgr">HuBERT-MGR</a></li>
<li><a class="reference internal" href="#unispeech-sat">Unispeech-SAT</a></li>
<li><a class="reference internal" href="#wavlm">WavLM</a></li>
<li><a class="reference internal" href="#data2vec">data2vec</a></li>
<li><a class="reference internal" href="#ast">AST</a></li>
<li><a class="reference internal" href="#ssast">SSAST</a></li>
<li><a class="reference internal" href="#mae-ast">MAE-AST</a></li>
<li><a class="reference internal" href="#byol-a">Byol-A</a></li>
<li><a class="reference internal" href="#byol-s">Byol-S</a></li>
<li><a class="reference internal" href="#vggish">VGGish</a></li>
<li><a class="reference internal" href="#passt">PaSST</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    <script src="../_static/js/custom.js"></script>
    </body>
</html>