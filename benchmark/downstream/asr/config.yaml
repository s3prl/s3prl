runner:
  total_steps: 200000
  gradient_clipping: 1
  gradient_accumulate_steps: 1

  log_step: 100
  eval_step: 2000
  save_step: 2000
  max_keep: 1
  eval_dataloaders:
    - dev

optimizer:
  name: AdamW
  lr: 2.0e-4

# comment the whole scheduler config block
# to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 1400

downstream_expert:
  data:
    corpus:                                 # Pass to dataloader
      # The following depends on corpus
      path: '/home/leo/d/datasets/LibriSpeech'
      name: 'LibriSpeech'
      train_split: ['train-clean-100']
      dev_split: ['dev-clean']
      bucketing: True
      batch_size: 16

    audio:                                  # Pass to audio transform
      feat_type: 'fbank'
      feat_dim:  40 
      apply_cmvn: False
      delta_order: 2                      # 0: do nothing, 1: add delta, 2: add delta and accelerate
      delta_window_size: 2
      frame_length: 25 # ms
      frame_shift: 10 # ms
      ref_level_db: 20
      min_level_db: -100
      preemphasis_coeff: 0.97
      augment: True
      time_aug: False
    text:
      mode: 'character'                     # 'character'/'word == phone'/'subword'
      vocab_file: 'benchmark/downstream/asr/corpus/librispeech_char.txt'

  hparas:                                   # Experiment hyper-parameters
    valid_step: 10
    max_step: 200000
    tf_start: 1.0
    tf_end: 1.0
    tf_step: 150000
    optimizer: 'Adadelta'
    lr: 1.0
    eps: 0.00000001                          # 1e-8
    lr_scheduler: 'self_defined'                    # 'fixed'/'warmup'
    curriculum: 0
    val_mode: 'wer'
    early_stopping: False
    label_smoothing: True

  model:                                    # Model architecture
    ctc_weight: 0.5                         # Weight for CTC loss
    encoder:
      vgg: 5                     # 4x reduction on time feature extraction ( 5: VGG + layer normalization)
      vgg_freq: -1
      vgg_low_filt: -1
      module: 'LSTM'                        # 'LSTM'/'GRU'/'Transformer'
      bidirection: True
      dim: [10, 10, 10, 10, 10]
      dropout: [0.3, 0.3,0.3,0.3,0.3]
      layer_norm: [False, False, False, False, False]
      proj: [True,True,True,True, True]           # Linear projection + Tanh after each rnn layer
      sample_rate: [1,1, 1,1,1]
      sample_style: 'drop'                  # 'drop'/'concat'
    attention:
      mode: 'loc'                           # 'dot'/'loc'
      dim: 300
      num_head: 1
      v_proj: False                         # if False and num_head>1, encoder state will be duplicated for each head
      temperature: 0.5                      # scaling factor for attention
      loc_kernel_size: 100                  # just for mode=='loc'
      loc_kernel_num: 10                    # just for mode=='loc'
    decoder:
      module: 'LSTM'                        # 'LSTM'/'GRU'/'Transformer'
      dim: 1024
      layer: 2
      dropout: 0
